{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eac8fc1d8846aeae2e516169ec27e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from testbed.models import Idefics\n",
    "import torch\n",
    "import config\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "model = Idefics(\n",
    "    config.idefics_9b_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import exp_settings as setting\n",
    "import datasets\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from testbed.data import prepare_caption_input, prepare_dataloader, prepare_vqa_input\n",
    "import config\n",
    "\n",
    "\n",
    "class ICVDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, lmm) -> None:\n",
    "        super().__init__()\n",
    "        self.lmm = lmm\n",
    "        tokenizer = self.lmm.processor.tokenizer\n",
    "        if tokenizer.sep_token is None:\n",
    "            tokenizer.add_special_tokens({\"sep_token\": \"[SEP]\"})\n",
    "            self.lmm.model.resize_token_embeddings(len(tokenizer))\n",
    "            self.lmm.model.tie_weights()\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            if setting.task == \"vqa\":\n",
    "                self.dataset = datasets.load_dataset(\n",
    "                    os.path.join(config.testbed_dir, \"data\", \"vqav2\"),\n",
    "                    split=\"train\",\n",
    "                    data_dir=config.vqav2_dir,\n",
    "                    images_dir=config.coco_dir,\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "            elif setting.task == \"caption\":\n",
    "                self.dataset = datasets.load_dataset(\n",
    "                    os.path.join(config.testbed_dir, \"data\", \"coco\"),\n",
    "                    data_dir=config.karpathy_coco_caption_dir,\n",
    "                    images_dir=config.coco_dir,\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        if setting.task == \"vqa\":\n",
    "            context, images = prepare_vqa_input(\n",
    "                batch, instruction=setting.vqa_instruction\n",
    "            )\n",
    "            # we use the first answer as grounding truth\n",
    "            answers = [item[-1][\"answers\"][0][\"answer\"] for item in batch]\n",
    "        elif setting.task == \"caption\":\n",
    "            context, images = prepare_caption_input(\n",
    "                batch, instruction=setting.caption_instruction\n",
    "            )\n",
    "            answers = [item[-1][\"sentences_raw\"][0] for item in batch]\n",
    "\n",
    "        # the last two items (take vqa as an example):\n",
    "        # [\n",
    "        #   { \"role\" : \"question\"\n",
    "        #     \"content\" :  ... },\n",
    "        #   { \"role\" : \"short answer\" }\n",
    "        # ]\n",
    "        query = [ctx[-2:] for ctx in context]\n",
    "        context = [ctx[:-2] for ctx in context]\n",
    "\n",
    "        context = self.lmm.apply_prompt_template(context)\n",
    "        query = self.lmm.apply_prompt_template(query)\n",
    "        # we use the sep token as delimiters to mark the boundary of examples, query and answer.\n",
    "        sep_token, sep_token_id, eos_token = (\n",
    "            self.lmm.processor.tokenizer.sep_token,\n",
    "            self.lmm.processor.tokenizer.sep_token_id,\n",
    "            self.lmm.processor.tokenizer.eos_token,\n",
    "        )\n",
    "        text_inputs = [\n",
    "            ctx + sep_token + q + sep_token + ans + eos_token\n",
    "            for ctx, q, ans in zip(context, query, answers)\n",
    "        ]\n",
    "\n",
    "        model_inputs = self.lmm.process_input(text_inputs, images)\n",
    "        # sep_token is only used to extract corresponding examples, qeury and answer\n",
    "        # it should be ignored in forward & backward process\n",
    "        model_inputs[\"attention_mask\"] = torch.where(\n",
    "            model_inputs[\"input_ids\"] == sep_token_id,\n",
    "            False,\n",
    "            model_inputs[\"attention_mask\"],\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return prepare_dataloader(\n",
    "            self.dataset,\n",
    "            setting.batch_size,\n",
    "            setting.num_shot,\n",
    "            collate_fn=self.collate_fn,\n",
    "            num_workers=setting.num_workers,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import exp_settings as setting\n",
    "from testbed.models.model_base import HookType\n",
    "\n",
    "\n",
    "class ICVModel(pl.LightningModule):\n",
    "    def __init__(self, lmm, icv_encoder: torch.nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.lmm = lmm\n",
    "\n",
    "        self.icv_encoder = icv_encoder\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: ICE [SEP] query [SEP] answer [EOS]\n",
    "\n",
    "        # step 1. ICV + query [SEP] answer [EOS]\n",
    "        hooks = self.lmm.register_forward_hook(\n",
    "            HookType.TEXT_MODEL_LAYER,\n",
    "            partial(self.icv_encoder.hook, model_inputs=inputs),\n",
    "        )\n",
    "        sep_token_id = self.lmm.processor.tokenizer.sep_token_id\n",
    "        sep_positions = (inputs['input_ids'] == sep_token_id).nonzero(as_tuple=True)[1]\n",
    "        query_sep_answer = inputs['input_ids'][:, sep_positions[0] + 1:]\n",
    "        query_outputs = self.lmm.model(**query_sep_answer, labels=query_sep_answer[\"input_ids\"])\n",
    "        ice_logits = query_outputs[\"logits\"]\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # step 2. ICE [SEP] query [SEP] answer [EOS]\n",
    "        with torch.no_grad():\n",
    "            icv_logits = self.lmm.model(**inputs)[\"logits\"]\n",
    "\n",
    "        tea_probs = ice_logits[icl_context_mask].softmax(dim=-1)\n",
    "        stu_log_probs = icv_logits[zero_shot_mask].log_softmax(dim=-1)\n",
    "        \n",
    "        kl_loss = F.kl_div(stu_log_probs, tea_probs, reduction='batchmean', log_target=False)\n",
    "\n",
    "        loss_dict = {\"kl_loss\": kl_loss}\n",
    "        loss += self.module_cfg.hard_loss_weight * query_outputs[\"loss\"]\n",
    "        loss_dict[\"ce_loss\"] = query_outputs[\"loss\"]\n",
    "        loss_dict[\"loss\"] = loss\n",
    "        return loss_dict, icv_encoder_output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss_dict, icv_encoder_output = self(**batch)\n",
    "        self.log_dict(loss_dict, sync_dist=True, prog_bar=True)\n",
    "        alpha = icv_encoder_output.alpha.squeeze()\n",
    "        for i in range(len(alpha)):\n",
    "            self.log(f\"alpha/alpha-{i}\", alpha[i])\n",
    "        return loss_dict[\"loss\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dict = {pn: p for pn, p in self.icv_encoder.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        decay_params = [\n",
    "            p for n, p in param_dict.items() if p.dim() >= 2 and \"alpha\" not in n\n",
    "        ]\n",
    "        nodecay_params = [\n",
    "            p for n, p in param_dict.items() if p.dim() < 2 and \"alpha\" not in n\n",
    "        ]\n",
    "\n",
    "        alpha_params = [p for n, p in param_dict.items() if \"alpha\" in n]\n",
    "\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": setting.weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "            {\n",
    "                \"params\": alpha_params,\n",
    "                \"weight_decay\": setting.weight_decay,\n",
    "                \"lr\": setting.alpha_lr,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = DeepSpeedCPUAdam(\n",
    "            optim_groups,\n",
    "            lr=setting.icv_lr,\n",
    "            weight_decay=setting.weight_decay,\n",
    "        )\n",
    "\n",
    "        step_batches = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = setting.warmup_step\n",
    "        if isinstance(warmup_steps, float):\n",
    "            warm_steps = warmup_steps * step_batches\n",
    "        elif isinstance(warmup_steps, int):\n",
    "            warm_steps = warmup_steps\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"the warm_steps should be int or float, but got {type(warmup_steps)}\"\n",
    "            )\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warm_steps, num_training_steps=step_batches\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n",
    "        }\n",
    "\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        params_name = list(checkpoint[\"state_dict\"].keys())\n",
    "        for name in params_name:\n",
    "            if name.startswith(\"model\"):\n",
    "                checkpoint[\"state_dict\"].pop(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    RichModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.utilities.deepspeed import (\n",
    "    convert_zero_checkpoint_to_fp32_state_dict,\n",
    ")\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "import config\n",
    "from testbed.models import Idefics\n",
    "from transformers import IdeficsModel\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    pl.seed_everything(436)\n",
    "    os.makedirs(config.result_dir, exist_ok=True)\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "\n",
    "    wb_logger = WandbLogger(\n",
    "        save_dir=config.result_dir,\n",
    "        name=\"trial\",\n",
    "        project=\"VQAInContextVector\",\n",
    "        log_model=False,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        logger=wb_logger,\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(),\n",
    "            RichModelSummary(max_depth=2),\n",
    "            RichProgressBar(),\n",
    "        ],\n",
    "        max_epochs=10,\n",
    "        strategy=\"deepspeed_stage_2_offload\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        gradient_clip_val=1.0,\n",
    "        log_every_n_steps=25,\n",
    "        accumulate_grad_batches=8,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    lmm = Idefics(config.idefics_9b_path, model_class=IdeficsModel)\n",
    "    data_module = ICVDataModule(\n",
    "        data_config=config.data_config,\n",
    "        prompt_manager=prompt_manager,\n",
    "        prompt_processor=processor,\n",
    "    )\n",
    "    model = ICVModel(\n",
    "        lmm=, icv_encoder=None\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        data_module,\n",
    "    )\n",
    "    trainer.save_checkpoint(\n",
    "        filepath=os.path.join(\n",
    "            save_path,\n",
    "            \"last.ckpt\",\n",
    "        ),\n",
    "        weights_only=True,\n",
    "    )\n",
    "    postprocess(config, save_path)\n",
    "\n",
    "\n",
    "@rank_zero_only\n",
    "def postprocess(config, save_path):\n",
    "    # TODO: Save layer map\n",
    "    save_path = Path(save_path)\n",
    "    if \"deepspeed\" in config.trainer.strategy:\n",
    "        cpk_save_path = save_path / \"last.ckpt\"\n",
    "        output_file = save_path / \"lightning_module.bin\"\n",
    "        convert_zero_checkpoint_to_fp32_state_dict(cpk_save_path, output_file)\n",
    "\n",
    "        checkpoint = torch.load(output_file)\n",
    "        params_name = list(checkpoint[\"state_dict\"].keys())\n",
    "        for name in params_name:\n",
    "            if \"lmm\" in name or \"interface.model\" in name:\n",
    "                checkpoint[\"state_dict\"].pop(name)\n",
    "        checkpoint[\"state_dict\"][\"use_sigmoid\"] = getattr(\n",
    "            config.icv_module.icv_encoder, \"use_sigmoid\", None\n",
    "        )\n",
    "        checkpoint[\"state_dict\"][\"lmm_args\"] = checkpoint[\"hyper_parameters\"][\n",
    "            \"lmm_config\"\n",
    "        ]\n",
    "        torch.save(checkpoint[\"state_dict\"], save_path / \"icv_cpk.pth\")\n",
    "        os.remove(output_file)\n",
    "        shutil.rmtree(\n",
    "            cpk_save_path,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

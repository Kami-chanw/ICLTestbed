{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model on Your Dataset\n",
    "\n",
    "In this tutorial, we will show step by step how to use `ICLTestbed` for model inference and evaluation on your own dataset.\n",
    "\n",
    "Let's take Mistral-7b-v0.3 as an example. Mistral is a 7-billion-parameter language model engineered for superior performance and efficiency. The more details about Mistral can be found in following ways:\n",
    "\n",
    "[paper](https://arxiv.org/abs/2310.06825) [blog](https://huggingface.co/mistralai/Mistral-7B-v0.3) [official-code](https://github.com/huggingface/transformers/tree/main/src/transformers/models/mistral)\n",
    "\n",
    "## Step 1. Data Loading\n",
    "Load dataset by `datasets` library. We first create a randomly generated Boolean expression dataset from [Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning](https://arxiv.org/abs/2410.04691v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['(False and True or False) and (True or False and False) or (False or False or True)',\n",
       "  '(True and False and False) or (False and False or False) or (False or True)',\n",
       "  '(True and True) or (True and True) and (True or True)',\n",
       "  '(False or False) and (True or False) or (False or False)',\n",
       "  '(False and True or True) or (False and True and True) and (True or False)'],\n",
       " 'answer': [True, True, True, False, True]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import datasets\n",
    "\n",
    "\n",
    "# Copied from https://github.com/MikaStars39/ICLvsFinetune/blob/main/src/generate_data.py\n",
    "def generate_boolean_expression(num_terms=3):\n",
    "    operators = [\"and\", \"or\"]\n",
    "    values = [\"True\", \"False\"]\n",
    "    expression = []\n",
    "\n",
    "    # Start with a random boolean value\n",
    "    expression.append(random.choice(values))\n",
    "\n",
    "    # Add operators and boolean values\n",
    "    for _ in range(num_terms - 1):\n",
    "        operator = random.choice(operators)\n",
    "        value = random.choice(values)\n",
    "        expression.append(operator)\n",
    "        expression.append(value)\n",
    "\n",
    "    # Join all parts to form the final expression\n",
    "    expression_str = \" \".join(expression)\n",
    "    return expression_str, eval(expression_str)\n",
    "\n",
    "\n",
    "def generate_bool_expression(\n",
    "    num_groups: int = 3,\n",
    "    num_terms: int = 4,\n",
    "    and_false: bool = False,\n",
    "    or_true: bool = False,\n",
    "    randoms: bool = False,\n",
    "    need_false: bool = False,\n",
    "):\n",
    "    if and_false == False and or_true == False and randoms == False:\n",
    "        choice = random.choice([\"False\", \"True\"])\n",
    "        if choice == \"False\":\n",
    "            and_false = True\n",
    "        else:\n",
    "            or_true = True\n",
    "\n",
    "    expression = []\n",
    "\n",
    "    for _ in range(num_groups):\n",
    "        # Determine the number of terms in this group\n",
    "        num_terms = random.randint(2, num_terms)\n",
    "        sub_expr, _ = generate_boolean_expression(num_terms)\n",
    "\n",
    "        # Add parentheses around the sub-expression\n",
    "        if len(expression) > 0:\n",
    "            operator = random.choice([\"and\", \"or\"])\n",
    "            expression.append(operator)\n",
    "        expression.append(f\"({sub_expr})\")\n",
    "\n",
    "    # Join all parts to form the final expression\n",
    "    expression_str = \" \".join(expression)\n",
    "\n",
    "    if and_false:\n",
    "        expression_str = \"(\" + expression_str + \")\" + \" and False\"\n",
    "    elif or_true:\n",
    "        expression_str = expression_str + \" or True\"\n",
    "\n",
    "    if need_false:\n",
    "        choice = random.choice([\"False\", \"True\"])\n",
    "        if choice == \"False\":\n",
    "            expression_str = \"(\" + expression_str + \")\" + \" or False\"\n",
    "        else:\n",
    "            expression_str = \"(\" + expression_str + \")\" + \" and True\"\n",
    "\n",
    "    return expression_str, eval(expression_str)\n",
    "\n",
    "\n",
    "def generate_dataset(\n",
    "    example_number: int,\n",
    "):\n",
    "    all_data = []\n",
    "    for _ in range(example_number):\n",
    "        question, answer = generate_bool_expression(randoms=True)\n",
    "        all_data.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "    return all_data\n",
    "\n",
    "dataset = datasets.Dataset.from_list(generate_dataset(200))\n",
    "dataset[range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from testbed.data import prepare_dataloader\n",
    "\n",
    "mistral_7b_path = \"/data/share/Mistral-7B\"\n",
    "\n",
    "hparams = {\n",
    "    \"batch_size\": 2,\n",
    "    \"num_shots\": 2,\n",
    "    \"dtype\": torch.float16,\n",
    "    \"generate_args\": {\"num_beams\": 3, \"max_new_tokens\": 5},\n",
    "}\n",
    "\n",
    "dataloader = prepare_dataloader(\n",
    "    dataset,\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    "    num_shots=hparams[\"num_shots\"],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Model Building\n",
    "The model in ICLTestbed can be roughly regarded as a simple combination of a processor and a specific model. You can access underlying processor or model by `model.processor` or `model.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ffb647168b4d728d929562ab7ff789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from testbed.models import Mistral\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Mistral(mistral_7b_path, torch_dtype=hparams[\"dtype\"]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Inference\n",
    "If you need to use your own prompt template, you should do it here. Suppose we want to use the following template:\n",
    "```\n",
    "Question: <question> Answer: <answer>\n",
    "```\n",
    "The prompt template in ICLTestbed is an alias for chat template from huggingface (not familiar? see [Chat Templating](https://huggingface.co/docs/transformers/main/chat_templating)). The model input here should usually be a `list` of `dict`, referred as `messages` in prompt template. For example, for a 1-shot context, \n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"role\": \"question\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"(True and False or True) and (False and False) or (False or False)\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"answer\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"False\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"(False or True) and (False and False) and (True and True)\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"answer\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "model.prompt_template =  (\n",
    "    \"{% if messages[0]['role'] == 'instruction' %}\"\n",
    "        \"Instruction: {{ messages[0]['content'] }}\\n\"\n",
    "        \"{% set messages = messages[1:] %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] != '' %}\"\n",
    "            \"{{ message['role'].capitalize() }}: \"\n",
    "        \"{%+ endif %}\"\n",
    "        \"{% if 'content' in message %}\"\n",
    "            \"{% for line in message['content'] %}\"\n",
    "                \"{% if line['type'] == 'text' %}\"\n",
    "                    \"{{ line['text'] }}\"\n",
    "                \"{% endif %}\"\n",
    "                \"{% if loop.last %}\"\n",
    "                    \"\\n\\n\"\n",
    "                \"{% endif %}\"\n",
    "            \"{% endfor %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to customize a prepare input to extract the data from the dataset and form the input of the model (see example above). Luckily, you can do this with the help of `register_dataset_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'instruction',\n",
       "   'content': 'Here are some boolean expressions, you need to directly tell the answer. If it is true, print True, else print False.'},\n",
       "  {'role': 'question',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '(False and True or False) and (True or False and False) or (False or False or True)'}]},\n",
       "  {'role': 'answer', 'content': [{'type': 'text', 'text': True}]},\n",
       "  {'role': 'question',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '(True and False and False) or (False and False or False) or (False or True)'}]},\n",
       "  {'role': 'answer', 'content': [{'type': 'text', 'text': True}]},\n",
       "  {'role': 'question',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '(True and True) or (True and True) and (True or True)'}]},\n",
       "  {'role': 'answer'}],\n",
       " [{'role': 'instruction',\n",
       "   'content': 'Here are some boolean expressions, you need to directly tell the answer. If it is true, print True, else print False.'},\n",
       "  {'role': 'question',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '(False or False) and (True or False) or (False or False)'}]},\n",
       "  {'role': 'answer', 'content': [{'type': 'text', 'text': False}]},\n",
       "  {'role': 'question',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '(False and True or True) or (False and True and True) and (True or False)'}]},\n",
       "  {'role': 'answer', 'content': [{'type': 'text', 'text': True}]},\n",
       "  {'role': 'question',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '(False and False) and (True or True) and (False or True)'}]},\n",
       "  {'role': 'answer'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testbed.data import register_dataset_retriever, prepare_input\n",
    "\n",
    "\n",
    "def retriever(item, is_last):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"question\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": item[\"question\"],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        (\n",
    "            {\"role\": \"answer\"}\n",
    "            if is_last\n",
    "            else {\n",
    "                \"role\": \"answer\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": item[\"answer\"]}],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "register_dataset_retriever(\"boolean\", retriever=retriever)\n",
    "prepare_input(\n",
    "    \"boolean\",\n",
    "    next(iter(dataloader)),\n",
    "    \"Here are some boolean expressions, you need to directly tell the answer. If it is true, print True, else print False.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It will be transformed to the real prompt by `model.apply_prompt_template` which is a step in `model.process_input`. `apply_prompt_template` is an alias for [`apply_chat_template`](https://huggingface.co/docs/transformers/main/chat_templating).\n",
    "\n",
    "After getting the model output, you need to do post-processing generation to clean and extract what answer should be. This is a dataset-dependent method, that is, different datasets have different post-processing styles. For our boolean expression dataset, just convert `True` to `1` and `False` to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question: (False and True or False) and (True or False and False) or (False or False or True)\\nAnswer: True\\nQuestion: (True and False and False) or (False and False or False) or (False or True)\\nAnswer: True\\nQuestion: (True and True) or (True and True) and (True or True)\\nAnswer: ']\n",
      "['True\\nQuestion: (']\n",
      "[1]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from testbed.data import register_postprocess, postprocess_generation\n",
    "\n",
    "register_postprocess(\"boolean\", lambda pred: int(eval(pred)))\n",
    "model.processor.pad_token = model.processor.eos_token\n",
    "batch = next(iter(dataloader))\n",
    "single_context = batch[0]\n",
    "text = prepare_input(\"boolean\", [single_context])\n",
    "print(model.apply_prompt_template(text))\n",
    "raw_output = model.generate(text, **hparams[\"generate_args\"])\n",
    "print(raw_output)\n",
    "prediction = postprocess_generation(\"boolean\", raw_output, stop_words=[\"\\n\", \"Question\", \"Answer\"])\n",
    "print(prediction)\n",
    "print(single_context[-1][\"answer\"])  # gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluate\n",
    "For our task, it uses ROC AUC to evaluate, which has already been implemented in [`evaluate`](https://huggingface.co/docs/evaluate/index) library that comes from hugging face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mistral-7b ...:   0%|          | 0/33 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:   3%|▎         | 1/33 [00:00<00:13,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:   6%|▌         | 2/33 [00:00<00:11,  2.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:   9%|▉         | 3/33 [00:01<00:11,  2.70it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  12%|█▏        | 4/33 [00:01<00:10,  2.78it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  15%|█▌        | 5/33 [00:01<00:09,  2.85it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  18%|█▊        | 6/33 [00:02<00:09,  2.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  21%|██        | 7/33 [00:02<00:09,  2.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  24%|██▍       | 8/33 [00:02<00:08,  2.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  27%|██▋       | 9/33 [00:03<00:08,  2.84it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating mistral-7b ...:  27%|██▋       | 9/33 [00:03<00:09,  2.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "total_roc_auc = evaluate.load(\"accuracy\")\n",
    "result = []\n",
    "\n",
    "# for simplicity, just run 10 batches\n",
    "for i, batch in zip(\n",
    "    range(10), tqdm(dataloader, desc=f\"Evaluating {model.model_name} ...\")\n",
    "):\n",
    "    text = prepare_input(\"boolean\", batch)\n",
    "    predictions = model.generate(text, **hparams[\"generate_args\"])\n",
    "    for pred, context in zip(predictions, batch):\n",
    "        last_item = context[-1]\n",
    "        answer = last_item[\"answer\"]\n",
    "        prediction = postprocess_generation(\"boolean\", pred, stop_words=[\"\\n\", \"Question\", \"Answer\"])\n",
    "        total_roc_auc.add(predictions=prediction, references=answer)\n",
    "        result.append(\n",
    "            {\n",
    "                \"question\": last_item[\"question\"],\n",
    "                \"answer\": last_item[\"answer\"],\n",
    "                \"raw_output\":pred,\n",
    "                \"prediction\": prediction,\n",
    "            }\n",
    "        )\n",
    "\n",
    "eval_result = total_roc_auc.compute()\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Save Results\n",
    "With the help of `evaluate.save`, we are able to save result and other hyper parameters to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams[\"dtype\"] = str(hparams[\"dtype\"])\n",
    "evaluate.save(\"./\", eval_result=eval_result, hparams=hparams, records=result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
